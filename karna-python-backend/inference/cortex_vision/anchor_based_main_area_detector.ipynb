{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add karna-python-backend to the path\n",
    "import sys\n",
    "sys.path.append('C:/Users/Prince/Documents/GitHub/Proejct-Karna/offline-ai-assistant/karna-python-backend')\n",
    "\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:16:41 INFO:NumExpr defaulting to 12 threads.\n",
      "02:16:46 INFO:Starting AnchorBasedMainAreaDetector test...\n",
      "02:16:46 INFO:Using viewport for cropping: {'x': 0, 'y': 121, 'width': 1920, 'height': 919}\n",
      "02:16:46 INFO:Loading data from: C:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\data\\chatgpt\\883c46f5-c62d-4799-baa1-5e3b12f12e8c\\screenshot_events_883c46f5-c62d-4799-baa1-5e3b12f12e8c.json\n",
      "02:16:46 INFO:Loading screenshot events from JSON file: C:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\data\\chatgpt\\883c46f5-c62d-4799-baa1-5e3b12f12e8c\\screenshot_events_883c46f5-c62d-4799-baa1-5e3b12f12e8c.json\n",
      "02:16:46 INFO:Created temporary directory for cropped images: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\n",
      "02:16:46 INFO:Loaded 4 valid mouse events with screenshots\n",
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "02:16:55 INFO:Parsing image path: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131725_617204.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omniparser initialized!!!\n",
      "image size: (1920, 919)\n",
      "\n",
      "0: 640x1280 13 icons, 68.0ms\n",
      "Speed: 15.6ms preprocess, 68.0ms inference, 46.9ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "len(filtered_boxes): 15 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "02:16:57 INFO:Created omniparser result for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Getting omniparser result model for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Converting parsed content df to bounding boxes for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Pre-processed parsed content results for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Creating omniparser result model for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Created omniparser result model for event_id: 9b073de5-e2c1-468f-a1ce-b7d033d4be7b\n",
      "02:16:57 INFO:Parsing image path: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131729_196616.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.2861800193786621\n",
      "image size: (1920, 919)\n",
      "\n",
      "0: 640x1280 14 icons, 54.5ms\n",
      "Speed: 11.3ms preprocess, 54.5ms inference, 11.1ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "len(filtered_boxes): 15 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "02:16:58 INFO:Created omniparser result for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Getting omniparser result model for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Converting parsed content df to bounding boxes for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Pre-processed parsed content results for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Creating omniparser result model for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Created omniparser result model for event_id: 2065c037-ad71-41ff-b4b7-72ad3cfbc481\n",
      "02:16:58 INFO:Parsing image path: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131757_316854.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.2489476203918457\n",
      "image size: (1920, 919)\n",
      "\n",
      "0: 640x1280 18 icons, 48.6ms\n",
      "Speed: 12.8ms preprocess, 48.6ms inference, 16.5ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "len(filtered_boxes): 21 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "02:17:00 INFO:Created omniparser result for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Getting omniparser result model for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Converting parsed content df to bounding boxes for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Pre-processed parsed content results for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Creating omniparser result model for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Created omniparser result model for event_id: bf685989-b84d-40ba-8287-5770a19a5f0c\n",
      "02:17:00 INFO:Parsing image path: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131801_573080.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.3354024887084961\n",
      "image size: (1920, 919)\n",
      "\n",
      "0: 640x1280 20 icons, 53.8ms\n",
      "Speed: 14.6ms preprocess, 53.8ms inference, 11.8ms postprocess per image at shape (1, 3, 640, 1280)\n",
      "len(filtered_boxes): 24 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "02:17:01 INFO:Created omniparser result for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Getting omniparser result model for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Converting parsed content df to bounding boxes for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Pre-processed parsed content results for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Creating omniparser result model for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Created omniparser result model for event_id: 2d269a28-6aac-4484-84e7-18e7cf719987\n",
      "02:17:01 INFO:Completed getting omniparser result models for 4 events\n",
      "02:17:01 INFO:Successfully loaded 4 models from JSON file\n",
      "02:17:01 INFO:Frame 0: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131725_617204.png\n",
      "02:17:01 INFO:  - Dimensions: 1920x919\n",
      "02:17:01 INFO:  - Elements: 15\n",
      "02:17:01 INFO:  - Element types: {'text': 2, 'icon': 13}\n",
      "02:17:01 INFO:  - Element sources: {'box_ocr_content_ocr': 2, 'box_yolo_content_ocr': 6, 'box_yolo_content_yolo': 7}\n",
      "02:17:01 INFO:Frame 1: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131729_196616.png\n",
      "02:17:01 INFO:  - Dimensions: 1920x919\n",
      "02:17:01 INFO:  - Elements: 15\n",
      "02:17:01 INFO:  - Element types: {'text': 1, 'icon': 14}\n",
      "02:17:01 INFO:  - Element sources: {'box_ocr_content_ocr': 1, 'box_yolo_content_ocr': 6, 'box_yolo_content_yolo': 8}\n",
      "02:17:01 INFO:Frame 2: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131757_316854.png\n",
      "02:17:01 INFO:  - Dimensions: 1920x919\n",
      "02:17:01 INFO:  - Elements: 21\n",
      "02:17:01 INFO:  - Element types: {'text': 5, 'icon': 16}\n",
      "02:17:01 INFO:  - Element sources: {'box_ocr_content_ocr': 5, 'box_yolo_content_ocr': 6, 'box_yolo_content_yolo': 10}\n",
      "02:17:01 INFO:Frame 3: C:\\Users\\Prince\\AppData\\Local\\Temp\\omni_viewport_11gattyh\\cropped_screenshot_20250329_131801_573080.png\n",
      "02:17:01 INFO:  - Dimensions: 1920x919\n",
      "02:17:01 INFO:  - Elements: 24\n",
      "02:17:01 INFO:  - Element types: {'text': 4, 'icon': 20}\n",
      "02:17:01 INFO:  - Element sources: {'box_ocr_content_ocr': 4, 'box_yolo_content_ocr': 7, 'box_yolo_content_yolo': 13}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.29138731956481934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:17:01 INFO:DynamicAreaDetector initialized with min_change_frequency>=0.3, min_area_size>=0.01, grouping_distance<=0.1\n",
      "02:17:01 INFO:UIOptimizedDynamicAreaDetector initialized with x_overlap_threshold=0.3, min_vertical_region_height=0.1\n",
      "02:17:03 INFO:AnchorBasedMainAreaDetector initialized with model resnet50\n",
      "02:17:03 INFO:Training the detector...\n",
      "02:17:03 INFO:--------------------------------------------------\n",
      "02:17:03 INFO:TRAINING PHASE\n",
      "02:17:03 INFO:--------------------------------------------------\n",
      "02:17:03 INFO:Training anchor-based main area detector using frames\n",
      "02:17:03 INFO:Training with image dimensions: 1920x919\n",
      "02:17:03 INFO:Using dynamic detector to find main content area\n",
      "02:17:03 INFO:Starting UI-optimized dynamic area detection for 4 frames\n",
      "02:17:03 INFO:Starting dynamic area detection for 4 frames\n",
      "02:17:03 INFO:Comparing frames 0 and 1\n",
      "02:17:04 INFO:Found 2 changes between frames 0 and 1\n",
      "02:17:04 INFO:Comparing frames 1 and 2\n",
      "02:17:04 INFO:Found 22 changes between frames 1 and 2\n",
      "02:17:04 INFO:Comparing frames 2 and 3\n",
      "02:17:05 INFO:Found 13 changes between frames 2 and 3\n",
      "02:17:05 INFO:Identified 17 dynamic regions from 3 frame pairs\n",
      "02:17:05 INFO:Dynamic area detection completed successfully\n",
      "02:17:05 INFO:UI-optimized dynamic area detection completed\n",
      "02:17:05 INFO:Detected main area from main_content_area: [531.0, 4.0, 1393.0, 878.0]\n",
      "02:17:05 INFO:Extracting patches for all UI elements\n",
      "02:17:05 INFO:Extracted patches for 15 elements\n",
      "02:17:05 INFO:Finding stable elements across frames\n",
      "02:17:06 INFO:Found 2 stable elements\n",
      "02:17:06 INFO:Identifying anchor points\n",
      "02:17:06 INFO:Selected 2 anchor points with direction distribution: {'top': 1, 'bottom': 1, 'left': 0, 'right': 0}\n",
      "02:17:06 INFO:Extracting main area references\n",
      "02:17:06 INFO:Created 4 main area references\n",
      "02:17:06 INFO:Saving model files to C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:06 INFO:Saved 2 anchor points and 4 main area references\n",
      "02:17:06 INFO:Saved model data to C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:06 INFO:Training successful. Found 2 anchor points.\n",
      "02:17:06 INFO:Detected main area: [531.0, 4.0, 1393.0, 878.0] (source: main_content_area)\n",
      "02:17:06 INFO:Main area dimensions: 862.0x874.0 pixels\n",
      "02:17:06 INFO:Main area center: (962.0, 441.0)\n",
      "02:17:06 INFO:Main area relative size: 42.70% of screen\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:06 INFO:ANCHOR POINTS DETAILS\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:06 INFO:Anchor points by direction: {'top': 1, 'bottom': 1, 'left': 0, 'right': 0}\n",
      "02:17:06 INFO:Anchor #0:\n",
      "02:17:06 INFO:  - Element ID: 3\n",
      "02:17:06 INFO:  - Element Type: text\n",
      "02:17:06 INFO:  - Source: box_ocr_content_ocr\n",
      "02:17:06 INFO:  - Direction: bottom\n",
      "02:17:06 INFO:  - Position: [825.0, 897.0, 1095.0, 913.0]\n",
      "02:17:06 INFO:  - Stability Score: 1.0000\n",
      "02:17:06 INFO:  - Horizontal Relation: left\n",
      "02:17:06 INFO:  - Vertical Relation: bottom\n",
      "02:17:06 INFO:  - Relative position to main area center: (-0.00, 0.53)\n",
      "02:17:06 INFO:Anchor #1:\n",
      "02:17:06 INFO:  - Element ID: 22\n",
      "02:17:06 INFO:  - Element Type: icon\n",
      "02:17:06 INFO:  - Source: box_yolo_content_yolo\n",
      "02:17:06 INFO:  - Direction: top\n",
      "02:17:06 INFO:  - Position: [0.0, 2.0, 9.0, 50.25]\n",
      "02:17:06 INFO:  - Stability Score: 0.9552\n",
      "02:17:06 INFO:  - Horizontal Relation: left\n",
      "02:17:06 INFO:  - Vertical Relation: top\n",
      "02:17:06 INFO:  - Relative position to main area center: (-1.11, -0.47)\n",
      "02:17:06 INFO:Extracting main area references\n",
      "02:17:06 INFO:Created 4 main area references\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:06 INFO:MAIN AREA REFERENCES\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:06 INFO:Number of main area references: 4\n",
      "02:17:06 INFO:Reference #0:\n",
      "02:17:06 INFO:  - Frame index: 0\n",
      "02:17:06 INFO:  - Bounding box: [531.0, 4.0, 1393.0, 878.0]\n",
      "02:17:06 INFO:  - Patch dimensions: 862x874\n",
      "02:17:06 INFO:Reference #1:\n",
      "02:17:06 INFO:  - Frame index: 1\n",
      "02:17:06 INFO:  - Bounding box: [531.0, 4.0, 1393.0, 878.0]\n",
      "02:17:06 INFO:  - Patch dimensions: 862x874\n",
      "02:17:06 INFO:Reference #2:\n",
      "02:17:06 INFO:  - Frame index: 2\n",
      "02:17:06 INFO:  - Bounding box: [531.0, 4.0, 1393.0, 878.0]\n",
      "02:17:06 INFO:  - Patch dimensions: 862x874\n",
      "02:17:06 INFO:Reference #3:\n",
      "02:17:06 INFO:  - Frame index: 3\n",
      "02:17:06 INFO:  - Bounding box: [531.0, 4.0, 1393.0, 878.0]\n",
      "02:17:06 INFO:  - Patch dimensions: 862x874\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:06 INFO:RUNTIME DETECTION PHASE\n",
      "02:17:06 INFO:--------------------------------------------------\n",
      "02:17:07 INFO:AnchorBasedMainAreaDetectorRuntime initialized with model resnet50\n",
      "02:17:07 INFO:Detecting main area in frame 0...\n",
      "02:17:07 INFO:Detecting main content area using model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:07 INFO:Loading model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:07 INFO:Loaded model (version 1.0.0) created at 2025-04-04T14:17:06.518101\n",
      "02:17:07 INFO:Loaded 2 anchor points with embeddings\n",
      "02:17:07 INFO:Loaded 4 main area references with embeddings\n",
      "02:17:07 INFO:Matching main area using sliding window approach\n",
      "02:17:07 INFO:Generated 130 candidate regions\n",
      "02:17:15 INFO:Main area matched with sliding window with high confidence\n",
      "02:17:15 INFO:  - Success: True\n",
      "02:17:15 INFO:  - Method: sliding_window_match\n",
      "02:17:15 INFO:  - Confidence: 0.9838\n",
      "02:17:15 INFO:  - Detected main area: [0, 0, 1920, 919]\n",
      "02:17:15 INFO:  - IoU with reference main area: 0.4270\n",
      "02:17:15 INFO:Detecting main area in frame 1...\n",
      "02:17:15 INFO:Detecting main content area using model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:15 INFO:Loading model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:15 INFO:Loaded model (version 1.0.0) created at 2025-04-04T14:17:06.518101\n",
      "02:17:15 INFO:Loaded 2 anchor points with embeddings\n",
      "02:17:15 INFO:Loaded 4 main area references with embeddings\n",
      "02:17:15 INFO:Matching main area using sliding window approach\n",
      "02:17:15 INFO:Generated 130 candidate regions\n",
      "02:17:23 INFO:Main area matched with sliding window with high confidence\n",
      "02:17:23 INFO:  - Success: True\n",
      "02:17:23 INFO:  - Method: sliding_window_match\n",
      "02:17:23 INFO:  - Confidence: 0.9835\n",
      "02:17:23 INFO:  - Detected main area: [0, 0, 1920, 919]\n",
      "02:17:23 INFO:  - IoU with reference main area: 0.4270\n",
      "02:17:23 INFO:Detecting main area in frame 2...\n",
      "02:17:23 INFO:Detecting main content area using model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:23 INFO:Loading model from C:\\Users\\Prince\\AppData\\Local\\Temp\\tmp381h552i\n",
      "02:17:23 INFO:Loaded model (version 1.0.0) created at 2025-04-04T14:17:06.518101\n",
      "02:17:23 INFO:Loaded 2 anchor points with embeddings\n",
      "02:17:23 INFO:Loaded 4 main area references with embeddings\n",
      "02:17:23 INFO:Matching main area using sliding window approach\n",
      "02:17:23 INFO:Generated 131 candidate regions\n",
      "02:17:30 INFO:Main area matched with sliding window with high confidence\n",
      "02:17:30 INFO:  - Success: True\n",
      "02:17:30 INFO:  - Method: sliding_window_match\n",
      "02:17:30 INFO:  - Confidence: 0.9565\n",
      "02:17:30 INFO:  - Detected main area: [522.4, 0, 1381.6, 211.55]\n",
      "02:17:30 INFO:  - IoU with reference main area: 0.2327\n",
      "02:17:30 INFO:Skipping remaining frames for brevity...\n",
      "02:17:30 INFO:--------------------------------------------------\n",
      "02:17:30 INFO:CREATING VISUALIZATIONS\n",
      "02:17:30 INFO:--------------------------------------------------\n",
      "02:17:31 INFO:Saved main area references visualization\n",
      "02:17:32 INFO:Saved direct matching visualization\n",
      "02:17:32 INFO:Saved anchor points visualization\n",
      "02:17:32 INFO:Saved anchor relationships visualization\n",
      "02:17:33 INFO:Saved reconstruction simulation visualization\n",
      "02:17:34 INFO:Saved detection workflow visualization\n",
      "02:17:34 INFO:Test completed. Created 8 visualization files:\n",
      "02:17:34 INFO:  - _anchor_patches.png\n",
      "02:17:34 INFO:  - _anchor_points.png\n",
      "02:17:34 INFO:  - _anchor_relationships.png\n",
      "02:17:34 INFO:  - _detection_workflow.png\n",
      "02:17:34 INFO:  - _direct_matching.png\n",
      "02:17:34 INFO:  - _main_area_location.png\n",
      "02:17:34 INFO:  - _main_area_references.png\n",
      "02:17:34 INFO:  - _reconstruction_simulation_only_anchors.png\n"
     ]
    }
   ],
   "source": [
    "from anchor_based_main_area_detector_test import test_anchor_based_detector\n",
    "test_anchor_based_detector(use_viewport=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
