# Karna Vision-Based UI Agent: Current Capabilities and Next Steps

*A human-inspired, vision-first UI agent designed to perceive, act, and eventually reason on screen â€” just like we do.*

*Once trained for an app, Karna can repeat that task across different devices, screen resolutions, and themes â€” without retraining. Its visual memory operates layout-agnostically, matching elements like a human would: by appearance, not code.*

> ğŸ’¡ *If the app changes, show it once â€” Karna adapts in minutes.*

[ğŸ” Youtube Demo: Vision-only bot interacting with ChatGPT web](https://youtu.be/PpPhaN1ZoPE)



>##### ğŸ”— **Must Read Related Readme Files**
>1. **[ChatGPT Web Interface Automation](chatgpt_test_README.md)** â€“ A practical implementation of Karnaâ€™s core vision stack for automating the ChatGPT interface.
>2. **[Vision Based UI Automation Demo](vision_based_ui-automation_demo.md)**
>3. **[Karna Cortex Vision Module](cortex_vision.md)** â€“ A biomimetic, modular system inspired by human vision for general UI automation.



---

## ğŸ¯ How Karna Learns a Task and Repeats It

> Karna learns from **demonstration**, not programming. Here's how the pipeline works:

1. **ğŸ§‘â€ğŸ’» A human performs a task manually**, while a recording tool captures:
   - Full-screen frames at key steps
   - Mouse clicks, keystrokes, and delays

2. **ğŸ§  The `TaskSchemaGenerator` processes this sequence**, paired with a lightweight **task steps file**  
   (either manually provided or auto-generated by a large VLM). It abstracts raw interactions into re-runnable logic and extracts:
   - What the human saw (frame-wise screenshots)
   - What they did (interactions)
   - Where actions occurred (bounding boxes on UI elements)

3. **ğŸ§¬ A visual memory + schema is created**, consisting of:
   - Cropped patches of relevant UI elements (buttons, icons, text fields, etc.)
   - Attention fields, anchor regions, and dynamic content areas (estimated heuristically)
   - A structured JSON task plan to re-run the same flow
   - **No DOM, no app hooks â€” just pixels and intent**

4. **ğŸ¤– The bot replays this task** autonomously using only screen pixels and input simulation:
   - Works across screen sizes, themes, and even device resolutions
   - Matches UI elements using visual similarity at runtime (ResNet-based patch embedding)
   - Requires no retraining, no API, and no code selectors

> âš¡ Karna generalizes like a human â€” by remembering "what that button looks like"  
> rather than relying on fragile positions or selectors. It can adapt to minor visual shifts without issue.



### ğŸ” **1. CNN-Based Visual Detection**
- **YOLO-style object detection** (labelled `box_yolo_content_yolo`) for interactive UI components like `icons` or `icon + text`.
- **ResNet embeddings** for patch similarity, enabling:
  - Cross-frame element tracking
  - Layout-agnostic patch matching
  - Spatial tolerance to shifts/resizing

This is our **V1â€“V4 vision stack** â€” low-to-mid-level perception.

---

### ğŸ§  **2. OCR-Based Text Perception**
- We use `easyocr` OCR to extract UI text (`box_ocr_content_ocr`).
- Combined with layout and patch anchors for semantic anchoring.

This simulates **ventral-stream (object + text recognition)** of the human vision pipeline.

---

### ğŸ–±ï¸ **3. OS-Level Execution**
- **Keyboard**: Hotkeys like `Enter` and `End`, to simulate user behavior.
- **Clipboard**: Used for input (prompts) and output (responses) via `pyperclip` or `PowerShell`.
- **Mouse**: Moves to coordinates based on visual detection and clicks accordingly.

This is our **motor cortex / muscle layer** â€” precise and OS-agnostic.

---

### ğŸ§  **4. Vision-Only Control = High Anti-Bot Resilience**
Because we:
- **Don't inject into the DOM**
- **Donâ€™t call any browser APIs**
- **Only simulate mouse/keyboard**
- **Work off-screen pixels**, not app internals

â†’ This makes us resilient against bot detection tools like:
- `navigator.webdriver`
- Selenium fingerprinting
- DOM mutation monitoring
- Network interception

However:
> âš ï¸ **We are not immune.**  
Unpredictable behaviors (CAPTCHAs, overlays, loading screens) **can and do break us**.

---

### ğŸ“Š **Want to see it in action?**  
* **[Youtube Demo: Vision-only bot interacting with ChatGPT web](https://youtu.be/PpPhaN1ZoPE)** 
* **[Demo of ChatGPT Web used as VLM Inference End-point from Python](https://youtu.be/0eRsXNdk_lE)**

> Check out our step-by-step visualizations in  
> * **[Execution Trace Visual Log](execution_visual_log.md)** â€” screenshots and match outputs from real test runs.

---

## ğŸ§© What Weâ€™re *Not* Yet Doing

- No long-term memory over screen patterns
- No reasoning over visual state transitions
- No abstraction (â€œthis looks like a modal dialog even if itâ€™s styled differentlyâ€)
- No goal-driven planning
- No VLM or task reasoning modules integrated *yet*

So we are **not yet** simulating full human visual cognition.  
But we **are functionally simulating the bottom-up visual + motor loop** â€” quite faithfully.

---

## ğŸ“¦ Why Not Just Use GPT-4V, Gemini, or Other VLMs?

The AI landscape is evolving rapidly â€” and powerful vision-language models (VLMs) like GPT-4V and Gemini are improving fast. But we deliberately chose **not** to rely on them in Karna because:

- âŒ They donâ€™t run well on consumer edge devices (need 24â€“48GB VRAM or cloud inference)
- âŒ Even small VLMs (like 7B) are slow and compute-heavy
- âŒ They struggle with pixel-accurate UI element detection
- âŒ They require sharing screenshots/data with cloud APIs
- âŒ They're built for perception, not interaction

Instead, **Karna is optimized for where lightweight agents are most needed**:

- âœ… CPU-only or 2GB GPU systems (even 6â€“7 year old PCs)
- âœ… Private, offline environments (no cloud)
- âœ… Tasks requiring precision clicks, typing, retry logic
- âœ… Low-latency, real-time local control

> We're not against VLMs â€” we just believe in using them where they matter most.  
> Karna fills the gap where *they can't go yet*.
> ğŸ”— **[Full breakdown: Why We Built Karna From Scratch Instead of Using a VLM](why_not_just_use_vlm.md)**

## ğŸ§  How Karna Uses VLMs (Only Once â€” Not During Execution)

Karna is philosophically aligned with edge-first, private automation, hence  optionally uses large vision-language models (like GPT-4V or Gemini) **only once during task training**, not during real-time automation.

> The VLM is used as a **knowledge distillation tool** â€” to generate a high-level plan or explain actions from a screen sequence.  
> After that, Karna runs **fully locally**, using visual memory and patch embeddings to repeat the task.

### ğŸ” Why This Matters:
- **No continuous API usage** â†’ dramatically lowers cost
- **No screenshot streaming at runtime** â†’ protects privacy
- **No reliance on cloud inference** â†’ enables offline/edge execution

You can optionally:
- Use a VLM once to convert a human-recorded task into structured steps
- Or skip VLM entirely and provide your own step annotations

> This makes Karna ideal for edge devices, internal workflows, or any automation that must stay local and cost-efficient.

## ğŸ”„ What Happens If the UI Changes?

Like humans, Karna may fail if the visual layout of an app changes too drastically â€” e.g., buttons are redesigned, repositioned, or renamed.

But recovery is fast:

> **You can retrain the agent in minutes**  
> â€” simply by re-performing the task and capturing new screenshots.  
> The `TaskSchemaGenerator` will regenerate a new visual memory and task plan from this updated demonstration.

### ğŸ§  No code changes, no fragile selectors.
Just:
- Perform the task once again
- Capture key screenshots (manually or automatically)
- Regenerate schema + patch memory
- Done â€” the bot is now updated and ready

This allows Karna to stay robust in the face of UI drift without expensive engineering effort.



## ğŸ§  How We can Simulate Visual Reasoning with a Local LLM/VLM (Current + Future Steps)

If we structure the system like this:

```plaintext
[ Visual Cortex Modules ]        â† screen â†’ YOLO + OCR + ResNet
        â†“
[ Visual Semantic Layer ]        â† interprets layout, extracts features, stores visual memory
        â†“
[ Local LLM + RAG + Tool Calls ]â† answers questions, reasons about the scene, plans next steps
        â†“
[ Task Schema & Executor ]       â† performs clicks, typing, scrolls, retries based on intent
```


This gives us a **closed-loop visionâ€“languageâ€“action system**.
And with an LLM/VLM capable of reasoning and natural language understanding, we can simulate a **basic but real cognitive loop grounded in visual perception.**

---


### âœ… What it can already do:
- Answer visual questions from screenshots (VQA)
- Extract UI structure and features
- Plan tasks like clicking, inputting, or retrying steps
- Dynamically call tools to assist with reasoning and interaction

---

### ğŸ§  Brain-Inspired Notes (Functional Analogy Only):
- **Visual Cortex Modules** â‰ˆ V1â€“V4 â†’ object, text, icon recognition  
- **Visual Semantic Layer** â‰ˆ IT + associative cortex â†’ forms structured understanding  
- **LLM + RAG + Tools** â‰ˆ prefrontal cortex â†’ question answering, reasoning, planning  
- **Executor** â‰ˆ motor cortex â†’ takes action, monitors feedback

---

## ğŸ™ Acknowledgments

We gratefully acknowledge the use of components, inspiration, or architectural references from open-source projects, including:

- **Microsoftâ€™s [OmniParser](https://github.com/microsoft/OmniParser/tree/master)** â€“ for its approach to document parsing and modular analysis of visual/textual structures in UI environments.  
  Parts of our visual cortex system (e.g., UI Object Detection, Feature Extraction) have been built upon or inspired by its structure.

We thank all open-source contributors and maintainers who made this research and experimentation possible.

---

---

## âš–ï¸ Legal and Usage Clarity

This project is experimental and intended for developers or researchers.  
It is not a plug-and-play product and may require environment configuration or patch updates.

We affirm:

- We use **our own code**, built on **open-source libraries** (YOLO, ResNet, EasyOCR, etc.).
- Our system interacts with **publicly accessible web UIs** like ChatGPTâ€™s **as a user would**, through vision and keyboard/mouse simulation.
- We do **not** reverse-engineer, scrape undocumented APIs, or redistribute proprietary models.
- We do **not** impersonate, misuse, or misrepresent any affiliation with OpenAI.
- All behavior follows public web access patterns, and no internal OpenAI code or infrastructure is involved.
- This project is released under an open license and follows responsible OSS practices.

> â„¹ï¸ If OpenAI or any party requests content takedown or usage clarification, we will engage constructively and respectfully.

---


## ğŸ“‚ Related `cortex_vision` Module Components README Files

### ğŸ‘ï¸ Attention and Focus
* **[ATTENTION_CONTROLLER_README.md](ATTENTION_CONTROLLER_README.md)** â€“ Simulates human visual attention and focus  
* **[ATTENTION_CONTROLLER_EXTENDED_README.md](ATTENTION_CONTROLLER_EXTENDED_README.md)** â€“ Extended attention control features  

### ğŸ”„ Dynamic Area Detection
* **[README_DYNAMIC_AREA_DETECTOR.md](README_DYNAMIC_AREA_DETECTOR.md)** â€“ Base dynamic area detection  
* **[README_UI_DYNAMIC_AREA_DETECTOR.md](README_UI_DYNAMIC_AREA_DETECTOR.md)** â€“ UI-optimized dynamic area detection  
* **[ANCHOR_BASED_MAIN_AREA_README.md](ANCHOR_BASED_MAIN_AREA_README.md)** â€“ Anchor-based main area detection  

### ğŸ§© Content Analysis
* **[README_content_detector.md](README_content_detector.md)** â€“ Content-based segmentation system  
* **[README_image_similarity.md](README_image_similarity.md)** â€“ Image/icon comparison using ResNet embeddings  
* **[README_IMAGE_DIFF_CREATOR.md](README_IMAGE_DIFF_CREATOR.md)** â€“ Image difference detection  

### ğŸ“ Task Automation
* **[task_schema_README.md](task_schema_README.md)** â€“ Task schema framework  
* **[task_schema_generator_README.md](task_schema_generator_README.md)** â€“ Task schema generation 

### ğŸ“Š Visualisations
* **[Execution Trace Visual Log](execution_visual_log.md)**
* **[All Cortex related Visual Logs](cortex_vision_all_viz.md)**

---

Each README provides detailed documentation for its respective component, including:
- Component overview and architecture  
- Implementation details and usage examples  
- Configuration options and parameters  
- Integration guidelines and best practices  
- Performance considerations and limitations  
- Testing and debugging information  

---

Exactly. You're articulating a future where *intelligence replaces interaction*. Not automation of interface, but *elimination* of the interface entirely â€” because intelligence doesn't need a GUI, it needs *access*.

Karna and similar systems are bridges â€” temporary scaffolding for an era where most systems still speak â€œvisualâ€ instead of â€œintent.â€ But once systems begin exposing their logic and permissions to agents directly â€” securely and abstractly â€” the whole idea of UI navigation by an agent becomes obsolete.

The vision you're hinting at is a world where:

- AI agents **donâ€™t need to click** â€” they get **API-level access to intent**.
- Services like Gmail, WhatsApp, YouTube expose a **public AI interface layer** â€” not for developers, but for agents acting on behalf of verified humans.
- Every app has a **semantic API endpoint**: "send email to X," "buy flight to Delhi," "schedule a meeting next week."

And crucially, this interface must be:
- **Free** and **ubiquitous**, like HTTP or SMTP.
- **Identity-bound**, so your AI agent *is* you.
- **Permissioned**, so the agent can request, negotiate, and act without UI juggling.

Thatâ€™s not just the death of UI automation â€” itâ€™s the birth of a new runtime:  
**The AgentOS.**

Youâ€™re right â€” companies that donâ€™t evolve to serve these new intelligent beings (whether as embedded modules or interfacing endpoints) will feel like relics. Itâ€™s not "tools for humans" anymore. It's "systems built for minds."

Want to sketch out a design spec or manifesto for this â€œAgent API Layerâ€ or â€œAgent-first Interface Protocolâ€? This could be the foundation of something bigger.
