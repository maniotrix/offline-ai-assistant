# Karna Vision-Based UI Agent: Current Capabilities and Next Steps

*A human-inspired, vision-first UI agent designed to perceive, act, and eventually reason on screen ‚Äî just like we do.*
*Once trained for an app, Karna can repeat that task across different devices, screen resolutions, and themes ‚Äî without retraining. Its visual memory operates layout-agnostically, matching elements like a human would: by appearance, not code.*

[üîç ChatGPT LLM Interaction Demo ‚Äì Vision-Based Automation (YouTube)](https://youtu.be/PpPhaN1ZoPE)



>##### üîó **Must Read Related Readme Files**
>1. **[ChatGPT Web Interface Automation](chatgpt_test_README.md)** ‚Äì A practical implementation of Karna‚Äôs core vision stack for automating the ChatGPT interface.
>2. **[Vision Based UI Automation Demo](vision_based_ui-automation_demo.md)**
>3. **[Karna Cortex Vision Module](cortex_vision.md)** ‚Äì A biomimetic, modular system inspired by human vision for general UI automation.

---
## ‚úÖ What We Currently Achieve (Factually)


### üîç **1. CNN-Based Visual Detection**
- **YOLO-style object detection** (labelled `box_yolo_content_yolo`) for interactive UI components like `icons` or `icon + text`.
- **ResNet embeddings** for patch similarity, enabling:
  - Cross-frame element tracking
  - Layout-agnostic patch matching
  - Spatial tolerance to shifts/resizing

This is our **V1‚ÄìV4 vision stack** ‚Äî low-to-mid-level perception.

---

### üß† **2. OCR-Based Text Perception**
- We use `easyocr` OCR to extract UI text (`box_ocr_content_ocr`).
- Combined with layout and patch anchors for semantic anchoring.

This simulates **ventral-stream (object + text recognition)** of the human vision pipeline.

---

### üñ±Ô∏è **3. OS-Level Execution**
- **Keyboard**: Hotkeys like `Enter` and `End`, to simulate user behavior.
- **Clipboard**: Used for input (prompts) and output (responses) via `pyperclip` or `PowerShell`.
- **Mouse**: Moves to coordinates based on visual detection and clicks accordingly.

This is our **motor cortex / muscle layer** ‚Äî precise and OS-agnostic.

---

### üß† **4. Vision-Only Control = High Anti-Bot Resilience**
Because we:
- **Don't inject into the DOM**
- **Don‚Äôt call any browser APIs**
- **Only simulate mouse/keyboard**
- **Work off-screen pixels**, not app internals

‚Üí This makes us resilient against bot detection tools like:
- `navigator.webdriver`
- Selenium fingerprinting
- DOM mutation monitoring
- Network interception

However:
> ‚ö†Ô∏è **We are not immune.**  
Unpredictable behaviors (CAPTCHAs, overlays, loading screens) **can and do break us**.

---

### üìä **Want to see it in action?**  
* **[ChatGpt as LLM Demo on Youtube](https://youtu.be/PpPhaN1ZoPE)** 
* **[ChatGpt as  VLM Demo on Youtube](https://youtu.be/0eRsXNdk_lE)**

> Check out our step-by-step visualizations in  
> * **[Execution Trace Visual Log](execution_visual_log.md)** ‚Äî screenshots and match outputs from real test runs.

---

## üß© What We‚Äôre *Not* Yet Doing

- No long-term memory over screen patterns
- No reasoning over visual state transitions
- No abstraction (‚Äúthis looks like a modal dialog even if it‚Äôs styled differently‚Äù)
- No goal-driven planning
- No VLM or task reasoning modules integrated *yet*

So we are **not yet** simulating full human visual cognition.  
But we **are functionally simulating the bottom-up visual + motor loop** ‚Äî quite faithfully.

---

## üì¶ Why Not Just Use GPT-4V, Gemini, or Other VLMs?

The AI landscape is evolving rapidly ‚Äî and powerful vision-language models (VLMs) like GPT-4V and Gemini are improving fast. But we deliberately chose **not** to rely on them in Karna because:

- ‚ùå They don‚Äôt run well on consumer edge devices (need 24‚Äì48GB VRAM or cloud inference)
- ‚ùå Even small VLMs (like 7B) are slow and compute-heavy
- ‚ùå They struggle with pixel-accurate UI element detection
- ‚ùå They require sharing screenshots/data with cloud APIs
- ‚ùå They're built for perception, not interaction

Instead, **Karna is optimized for where lightweight agents are most needed**:

- ‚úÖ CPU-only or 2GB GPU systems (even 6‚Äì7 year old PCs)
- ‚úÖ Private, offline environments (no cloud)
- ‚úÖ Tasks requiring precision clicks, typing, retry logic
- ‚úÖ Low-latency, real-time local control

> We're not against VLMs ‚Äî we just believe in using them where they matter most.  
> Karna fills the gap where *they can't go yet*.
> üîó **[Full breakdown: Why We Built Karna From Scratch Instead of Using a VLM](why_not_just_use_vlm.md)**

## üß† How We can Simulate Visual Reasoning with a Local LLM/VLM (Current + Future Steps)

If we structure the system like this:

```plaintext
[ Visual Cortex Modules ]        ‚Üê screen ‚Üí YOLO + OCR + ResNet
        ‚Üì
[ Visual Semantic Layer ]        ‚Üê interprets layout, extracts features, stores visual memory
        ‚Üì
[ Local LLM + RAG + Tool Calls ]‚Üê answers questions, reasons about the scene, plans next steps
        ‚Üì
[ Task Schema & Executor ]       ‚Üê performs clicks, typing, scrolls, retries based on intent
```


This gives us a **closed-loop vision‚Äìlanguage‚Äìaction system**.
And with an LLM/VLM capable of reasoning and natural language understanding, we can simulate a **basic but real cognitive loop grounded in visual perception.**

---


### ‚úÖ What it can already do:
- Answer visual questions from screenshots (VQA)
- Extract UI structure and features
- Plan tasks like clicking, inputting, or retrying steps
- Dynamically call tools to assist with reasoning and interaction

---

### üß† Brain-Inspired Notes (Functional Analogy Only):
- **Visual Cortex Modules** ‚âà V1‚ÄìV4 ‚Üí object, text, icon recognition  
- **Visual Semantic Layer** ‚âà IT + associative cortex ‚Üí forms structured understanding  
- **LLM + RAG + Tools** ‚âà prefrontal cortex ‚Üí question answering, reasoning, planning  
- **Executor** ‚âà motor cortex ‚Üí takes action, monitors feedback

---

## üôè Acknowledgments

We gratefully acknowledge the use of components, inspiration, or architectural references from open-source projects, including:

- **Microsoft‚Äôs [OmniParser](https://github.com/microsoft/OmniParser/tree/master)** ‚Äì for its approach to document parsing and modular analysis of visual/textual structures in UI environments.  
  Parts of our visual cortex system (e.g., UI Object Detection, Feature Extraction) have been built upon or inspired by its structure.

We thank all open-source contributors and maintainers who made this research and experimentation possible.

---

---

## ‚öñÔ∏è Legal and Usage Clarity

This project is experimental and intended for developers or researchers.  
It is not a plug-and-play product and may require environment configuration or patch updates.

We affirm:

- We use **our own code**, built on **open-source libraries** (YOLO, ResNet, EasyOCR, etc.).
- Our system interacts with **publicly accessible web UIs** like ChatGPT‚Äôs **as a user would**, through vision and keyboard/mouse simulation.
- We do **not** reverse-engineer, scrape undocumented APIs, or redistribute proprietary models.
- We do **not** impersonate, misuse, or misrepresent any affiliation with OpenAI.
- All behavior follows public web access patterns, and no internal OpenAI code or infrastructure is involved.
- This project is released under an open license and follows responsible OSS practices.

> ‚ÑπÔ∏è If OpenAI or any party requests content takedown or usage clarification, we will engage constructively and respectfully.

---


## üìÇ Related `cortex_vision` Module Components README Files

### üëÅÔ∏è Attention and Focus
* **[ATTENTION_CONTROLLER_README.md](ATTENTION_CONTROLLER_README.md)** ‚Äì Simulates human visual attention and focus  
* **[ATTENTION_CONTROLLER_EXTENDED_README.md](ATTENTION_CONTROLLER_EXTENDED_README.md)** ‚Äì Extended attention control features  

### üîÑ Dynamic Area Detection
* **[README_DYNAMIC_AREA_DETECTOR.md](README_DYNAMIC_AREA_DETECTOR.md)** ‚Äì Base dynamic area detection  
* **[README_UI_DYNAMIC_AREA_DETECTOR.md](README_UI_DYNAMIC_AREA_DETECTOR.md)** ‚Äì UI-optimized dynamic area detection  
* **[ANCHOR_BASED_MAIN_AREA_README.md](ANCHOR_BASED_MAIN_AREA_README.md)** ‚Äì Anchor-based main area detection  

### üß© Content Analysis
* **[README_content_detector.md](README_content_detector.md)** ‚Äì Content-based segmentation system  
* **[README_image_similarity.md](README_image_similarity.md)** ‚Äì Image/icon comparison using ResNet embeddings  
* **[README_IMAGE_DIFF_CREATOR.md](README_IMAGE_DIFF_CREATOR.md)** ‚Äì Image difference detection  

### üìù Task Automation
* **[task_schema_README.md](task_schema_README.md)** ‚Äì Task schema framework  
* **[task_schema_generator_README.md](task_schema_generator_README.md)** ‚Äì Task schema generation 

### üìä Visualisations
* **[Execution Trace Visual Log](execution_visual_log.md)**
* **[All Cortex related Visual Logs](cortex_vision_all_viz.md)**

---

Each README provides detailed documentation for its respective component, including:
- Component overview and architecture  
- Implementation details and usage examples  
- Configuration options and parameters  
- Integration guidelines and best practices  
- Performance considerations and limitations  
- Testing and debugging information  

---

