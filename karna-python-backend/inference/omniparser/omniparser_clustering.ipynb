{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:12:22 INFO:NumExpr defaulting to 12 threads.\n"
     ]
    }
   ],
   "source": [
    "#  add karna-python-backend to the path\n",
    "import sys\n",
    "sys.path.append('C:/Users/Prince/Documents/GitHub/Proejct-Karna/offline-ai-assistant/karna-python-backend')\n",
    "sys.path.append('C:/Users/Prince/Documents/GitHub/Proejct-Karna/offline-ai-assistant/karna-python-backend/inference')\n",
    "\n",
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from clustering import *\n",
    "from config.paths import workspace_dir, backend_inference_dir\n",
    "from omniparser.omni_helper import OmniParserResultModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omniparser initialized!!!\n",
      "image size: (1920, 1080)\n",
      "\n",
      "0: 736x1280 87 icons, 82.8ms\n",
      "Speed: 15.7ms preprocess, 82.8ms inference, 52.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "len(filtered_boxes): 91 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prince\\Documents\\GitHub\\Proejct-Karna\\offline-ai-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "03:12:45 INFO:Getting omniparser result model for event_id: -1\n",
      "03:12:45 INFO:Converting parsed content df to bounding boxes for event_id: -1\n",
      "03:12:45 INFO:Creating omniparser result model for event_id: -1\n",
      "03:12:45 INFO:Created omniparser result model for event_id: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.7126832008361816\n"
     ]
    }
   ],
   "source": [
    "# get omniparser result from image path\n",
    "import os\n",
    "from omniparser.omni_helper import get_omniparser_inference_data_from_image_path\n",
    "# get current directory\n",
    "omniparser_dir = os.path.join(backend_inference_dir, \"omniparser\")\n",
    "image_path = os.path.join(omniparser_dir, \"cluster_test.png\")\n",
    "omniparser_result_model : OmniParserResultModel = get_omniparser_inference_data_from_image_path(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create cluster preprocessor\n",
    "parsed_content_results : List[ParsedContentResult] = omniparser_result_model.parsed_content_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_model_heirarchy = ClusterModelHeirarchy(os.path.join(omniparser_dir, \"sample_cluster_rules.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_preprocessor = ClusterPreprocessor(parsed_content_results, \n",
    "                                            cluster_model_heirarchy, \n",
    "                                            omniparser_result_model.omniparser_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterResult(cluster_result_id='v0_h0', parsed_content_results=[ParsedContentResult(type='icon', bbox=[12.0, 129.0, 52.0, 168.0], interactivity=True, content='@ ', source='box_yolo_content_ocr', id=13), ParsedContentResult(type='icon', bbox=[53.0, 130.0, 92.0, 167.0], interactivity=True, content='( ', source='box_yolo_content_ocr', id=14)])\n",
      "ClusterResult(cluster_result_id='v0_h1', parsed_content_results=[ParsedContentResult(type='icon', bbox=[92.0, 130.0, 229.0, 165.0], interactivity=True, content='ChatGPT 40 ', source='box_yolo_content_ocr', id=16)])\n",
      "ClusterResult(cluster_result_id='v0_h2', parsed_content_results=[ParsedContentResult(type='icon', bbox=[1734.0, 128.0, 1859.0, 169.0], interactivity=True, content='Temporary ', source='box_yolo_content_ocr', id=9)])\n",
      "ClusterResult(cluster_result_id='v0_h3', parsed_content_results=[ParsedContentResult(type='icon', bbox=[1864.0, 129.0, 1903.0, 169.0], interactivity=True, content='PowerPoint', source='box_yolo_content_yolo', id=44)])\n",
      "ClusterResult(cluster_result_id='v1_h0', parsed_content_results=[ParsedContentResult(type='icon', bbox=[619.0, 291.0, 1337.0, 563.0], interactivity=True, content=\"Temporary Chat This chat won't appear in history; use or create memories; or be used to train our models: For safety purposes, we may keep copy for up to 30 days: \", source='box_yolo_content_ocr', id=17)])\n",
      "ClusterResult(cluster_result_id='v2_h0', parsed_content_results=[ParsedContentResult(type='text', bbox=[602.0, 576.0, 698.0, 602.0], interactivity=False, content='Ask anything', source='box_ocr_content_ocr', id=6), ParsedContentResult(type='icon', bbox=[636.0, 616.0, 723.0, 655.0], interactivity=True, content='Search ', source='box_yolo_content_ocr', id=10), ParsedContentResult(type='icon', bbox=[592.0, 616.0, 631.0, 655.0], interactivity=True, content='Add', source='box_yolo_content_yolo', id=34)])\n",
      "ClusterResult(cluster_result_id='v2_h1', parsed_content_results=[ParsedContentResult(type='icon', bbox=[727.0, 616.0, 766.0, 656.0], interactivity=True, content='More options', source='box_yolo_content_yolo', id=33)])\n",
      "ClusterResult(cluster_result_id='v2_h2', parsed_content_results=[ParsedContentResult(type='icon', bbox=[1294.0, 611.0, 1333.0, 655.0], interactivity=True, content='Up arrow', source='box_yolo_content_yolo', id=55)])\n",
      "ClusterResult(cluster_result_id='v3_h0', parsed_content_results=[ParsedContentResult(type='text', bbox=[825.0, 1017.0, 1095.0, 1033.0], interactivity=False, content='ChatGPT can make mistakes Check important info.', source='box_ocr_content_ocr', id=7)])\n",
      "ClusterResult(cluster_result_id='v3_h1', parsed_content_results=[ParsedContentResult(type='icon', bbox=[1881.0, 1000.0, 1911.0, 1034.0], interactivity=True, content='Help', source='box_yolo_content_yolo', id=77)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create cluster worker\n",
    "cluster_worker = ClusterDBSCANWorker(cluster_preprocessor)\n",
    "# cluster\n",
    "cluster_results = cluster_worker.cluster()\n",
    "\n",
    "# print the cluster results\n",
    "for cluster_result in cluster_results:\n",
    "    print(cluster_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
